services:
  kafka:
    image: confluentinc/cp-kafka:7.5.0
    container_name: kafka
    ports:
      - "9092:9092"
    environment:
      KAFKA_NODE_ID: 1
      KAFKA_PROCESS_ROLES: broker,controller
      KAFKA_LISTENERS: PLAINTEXT://0.0.0.0:9092,CONTROLLER://0.0.0.0:9093
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://localhost:9092
      KAFKA_CONTROLLER_LISTENER_NAMES: CONTROLLER
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT
      KAFKA_CONTROLLER_QUORUM_VOTERS: 1@kafka:9093
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1
      KAFKA_LOG_DIRS: /tmp/kraft-combined-logs
      CLUSTER_ID: MkU3OEVBNTcwNTJENDM2Qk

  # spark: MIS EN LOCAL CAR PB D'IMAGE AVEC DOCKER
  #   image: apache/spark-py:v3.5.0
  #   container_name: spark-streaming
  #   user: root
  #   ports:
  #     - "8080:8080" # Spark Web UI
  #     - "4040:4040"
  #     - "7077:7077" # Spark Master Port
  #   volumes:
  #     - ./data:/opt/spark/data
  #     - ./checkpoints:/opt/spark/checkpoints
  #     - ./logs:/opt/spark/logs
  #   command: >
  #     /bin/bash -c "
  #     /opt/spark/sbin/start-master.sh -h 0.0.0.0 &&
  #     tail -f /opt/spark/logs/spark*.out
  #     "
  #   working_dir: /opt/spark


# Petite explication des services :
# Imaginez que vous venez de monter une usine de traitement de données :

# Kafka = Le convoyeur à l'entrée qui reçoit les colis (messages)
# Spark = La chaîne de production qui transforme les colis
# Delta Lake = L'entrepôt où on stocke les produits finis

# Votre docker-compose.yml = le plan de construction de l'usine